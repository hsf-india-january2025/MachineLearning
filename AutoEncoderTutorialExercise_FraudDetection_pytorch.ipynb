{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Autoencoder for credit card detection (Pytorch)\n\nThe objective of this exercice is to build a model able to detect fraudulous credit card transactions among normal transactions. For this we train a special type of neural network called autoencoder. This network has as many input nodes as output nodes, and several hidden layers with, usually lower dimensions.\n\nThe dataset we're going to use can be downloaded from [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud) (big file: 68 MB). It contains data about credit card transactions that occurred during a period of two days, with 492 frauds out of 284,807 transactions.\n\nAll 30 features in the dataset are numerical. The data has been transformed using PCA transformation(s) due to privacy reasons. The two features that haven't been changed are Time and Amount. Time contains the seconds elapsed between each transaction and the first transaction in the dataset.\n\nThe dataset also contains the class of event: 0 = normal transaction; 1 = fraudulous transaction.\n\nSee example here:\n\n* This example: https://github.com/curiousily/Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras/blob/master/fraud_detection.ipynb\n\n* Another example: https://github.com/GitiHubi/deepAI/blob/master/GTC_2018_Lab.ipynb\n","metadata":{"id":"KGad4xJwsr7A"}},{"cell_type":"markdown","source":"## Initialize","metadata":{"id":"YWiPGN8Csr7E"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.utils import shuffle\n\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport torch\nimport torch.nn as nn\n\nimport sys\nprint(sys.version)\nprint(torch.version)\nprint('cuda:',torch.version.cuda)\nprint('\\nRun on cpu')\ndtype = torch.FloatTensor\ndevice = torch.device(\"cpu\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihJVQAV4sr7G","outputId":"bce19d83-4785-4f1d-e8c4-74de4750de9d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Explore Data\n\na) Download the dataset and load it in a panda dataframe. Look at the first 10 examples.\n\nb) Separate the data in two classes `normal` and `fraud`, then remove the class label from these datasets in order to conserve only the features.\n\nc) Plot the first 5 features of both normal and fraud data (plotting all features is time consuming).\n\nd) Split the `normal` dataset into a training and a test sample (each of same size).\n\nAfter the last step you should have 3 datasets:\n* normal data used for training\n* normal data used for testing\n* fraud data used for testing","metadata":{"id":"kJNiGovGsr7I"}},{"cell_type":"code","source":"# Load data\n\nurl = \"https://cernbox.cern.ch/remote.php/dav/public-files/eebqSB1kYTMZ3Wm/creditcard.csv\"\ndf = pd.read_csv(url)\n\n# Print first 10 entries\nprint(df[:10])\n\n# Get events for each class\n#--------------------------\nfraud  = df[df['Class']==1]  # Fraud\nnormal = df[df['Class']==0]  # Normal transactions\n\n# Remove target (not needed)\ndrop=['Class']\nfraud  = fraud.drop(columns=drop)\nnormal = normal.drop(columns=drop)\n\nvarname = list(normal)\n\n# Split normal class in two parts (train and test)\nx_train_normal,x_test_normal = train_test_split(normal.values,test_size=0.5,random_state=13)\n\n# test fraud\nx_test_fraud = fraud.values\n\nprint('x_train=', x_train_normal.shape,  'x_test_back=', x_test_normal.shape, 'x_test_signal=', x_test_fraud.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcezuGSWsr7K","outputId":"71ddb50b-b888-4f23-bbca-30ea81bfccca","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Look at data","metadata":{"id":"Bjt7EkfCsr7M"}},{"cell_type":"code","source":"# Plotting original images\n#bins = np.arange(-2, 2, 0.01)\n\nprint(\"Original Images\")\nplt.figure(figsize=(20, 18))\nfor i in range(5):\n    plt.subplot(5, 6, i+1)\n    n, bins, patches = plt.hist(x_test_normal[:,i], bins='auto', color = 'orange', alpha=0.5, label='Normal', density=1)\n    plt.hist(x_test_fraud[:,i], bins, alpha=0.5, label='Fraud', density=1)\n    plt.xlabel(varname[i])\n    plt.legend(loc='best')\n\nplt.savefig('AE_fraud_detection_original_variables_all.png')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"LGY3DFZXsr7N","outputId":"6fab60ca-64ab-4b1f-cf5e-bd4df6ce73e0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Rescale data\n\nSince features have different range we apply a transformation to each feature. For this we  use the MinMaxScaler that scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one:\n\nSee: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n\na) Fit and transform the training dataset using the scaler with the `fit_transform` method.\n\nb) Apply the transformation on the tests samples using the `transform` method.\n\nc) Plot the first 5 features of the normal and fraud test data and see how they changed.","metadata":{"id":"9TWCYw31sr7O"}},{"cell_type":"code","source":"# Rescale data\n#-------------\nscaler = MinMaxScaler()\n\n# fit (get min and max) and transform x_train\nx_train_normal = scaler.fit_transform(x_train_normal)\n\n# transform x_test (using scale parameters of x_train)\nx_test_normal = scaler.transform(x_test_normal)\nx_test_fraud = scaler.transform(x_test_fraud)","metadata":{"id":"7C0b9Li7sr7P","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show variables after rescaling\n\nprint(\"Rescaled Images\")\nplt.figure(figsize=(20, 18))\nfor i in range(5):\n    plt.subplot(5, 6, i+1)\n    n, bins, patches = plt.hist(x_test_normal[:,i], bins='auto', color = 'orange', alpha=0.5, label='Normal', density=1)\n    plt.hist(x_test_fraud[:,i], bins, alpha=0.5, label='Fraud', density=1)\n    plt.xlabel(varname[i])\n    plt.legend(loc='best')\n\nplt.savefig('AE_fraud_detection_rescaled_variables_all.png')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"DZhWc3MYsr7R","outputId":"356b5eef-495e-491f-9f3b-9f430dd69ca1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Partition training data\n\nAfter all of this, it's important to partition the data. In order for your model to generalize well, you split the training data into two parts: a training and a validation set. You will train your model on 80% of the data and validate it on 20% of the remaining training data.","metadata":{"id":"rW9BN7DRsr7S"}},{"cell_type":"code","source":"x_train_train,x_train_valid = train_test_split(x_train_normal,test_size=0.2,random_state=13)\nprint(x_train_train.shape)\nprint(x_train_valid.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtT9x_jrsr7T","outputId":"7c080702-1ad4-4d39-af94-87d404b9cc62","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. AutoEncoder model\n\nNow we create the AutoEncoder model.\n\nComplete the network structure below using linear functions `nn.Linear(dim1,dim2)` (where `dim1` is the input dim of the layer and `dim2` the dimension of the layer output) and sigmoid activation functions `nn.Sigmoid()`:\n\na) in the encoding part create layers of dimension 30 (input) - 30 (hidden layer 1) - 25 (hidden layer 2) - 20 (latence space), each with a sigmoid activation function\n\nb) in the decoding part create layers of dimension 25 (hidden layer 2) - 30 (hidden layer 1) - 30 (output) , where only the 1st layer has a sigmoid activation function\n\nc) look at the forward function, what does it return ?","metadata":{"id":"plPHkVSwsr7U"}},{"cell_type":"markdown","source":"### Hyperparameters of the network","metadata":{"id":"sJvYhlTjsr7V"}},{"cell_type":"code","source":"num_epochs = 200\nbatch_size = 2048\nhidden_layer1 = 30\nhidden_layer2 = 25\nencoding_dim = 20","metadata":{"id":"lHMltBe7sr7W","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### AutoEncoder structure","metadata":{"id":"cQN3DdBosr7W"}},{"cell_type":"code","source":"input_dim = x_train_train.shape[1]\n\nclass autoencoder(nn.Module):\n    def __init__(self):\n        super(autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_layer1),\n            nn.Sigmoid(),\n            nn.Linear(hidden_layer1, hidden_layer2),\n            nn.Sigmoid(),\n            nn.Linear(hidden_layer2, encoding_dim),\n            nn.Sigmoid()\n        )\n        self.decoder = nn.Sequential(\n            # EXERCISE: Can you try to build the decoder?\n            # Hint: Think about the autoencoder structure!\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\nmodel = autoencoder()\n# Let's confirm that our model was build correctly\nprint(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"Jujeqpgxsr7X","outputId":"ddc13e0b-acd0-4e48-e0cc-870febcdfa7f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Set the data loading utilities\n\nWe now call the [DataLoader](https://pytorch.org/docs/stable/data.html) constructors for the following datasets:\n* normal data used for training\n* normal data used for validation\n* normal data used for testing\n* fraud data used for testing\n\nWe shuffle the loading process of the train and validation datasets to make the learning process independent of data orderness, but the order of test datasets remains the same to examine whether we can handle unspecified bias order of inputs.\n\nSee how this is done below (you need to replace your own dataset names where appropriate).","metadata":{"id":"F9OCuKbksr7Y"}},{"cell_type":"code","source":"# For training on normal samples\ntrain_loader = torch.utils.data.DataLoader(dataset=x_train_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\nvalid_loader = torch.utils.data.DataLoader(dataset=x_train_valid,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\n# For testing on fraud examples (shuffle=False)\ntest_fraud_loader = torch.utils.data.DataLoader(dataset=x_test_fraud,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n# For testing on unseen normal sample (shuffle=False)\n# EXERCISE: How can we define it?\n# test_normal_loader = ","metadata":{"id":"JJGP5ADIsr7Y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Training on normal samples\n\nRun the training of the network on the training sample. For this complete the code below by answering the following questions:\n\na) Choose the mean square error loss function. See https://pytorch.org/docs/master/nn.html#loss-functions\n\nb) Select the Adam optimizer (= minimization) method with a learning rate of 0.001. See https://pytorch.org/docs/stable/optim.html.\n\nc) Fill the validation step knowing that it is the same structure as the training step but without the minimization part (not needed for validation).\n\nd) Record for each epoch the loss value calculated for the training and validation steps. Make a figure of the training and validation losses as a function of the number of epochs. Do the two curve agree?\n","metadata":{"id":"WTAHj_Cysr7Z"}},{"cell_type":"code","source":"# Loss function: https://pytorch.org/docs/master/nn.html#loss-functions\ncriterion = nn.MSELoss()\n\n# Optimizer: https://pytorch.org/docs/stable/optim.html\nlearning_rate = 0.001  # default is 0.001\nl2_norm = 0\n\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate, weight_decay = l2_norm)\n\n# Train\ntrain_loss = []\nvalid_loss = []\n\nfor epoch in range(num_epochs):\n\n    ###################\n    # train the model #\n    ###################\n    model.train() # prep model for training\n    for data in train_loader:\n        data = data.type(dtype)\n        output = model(data)\n        loss = criterion(output, data)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # record train loss\n    train_loss.append(loss.item())\n\n    ######################\n    # validate the model #\n    ######################\n    model.eval() # prep model for evaluation\n    for data in valid_loader:\n        data = data.type(dtype)\n        output = model(data)\n        loss = criterion(output, data)\n\n    # record validation loss\n    valid_loss.append(loss.item())\n\n    if (epoch==0 or (epoch+1)%50==0):\n        print('epoch [{}/{}], train loss:{:.4f}, validation loss:{:.4f}'.format(epoch + 1, num_epochs, train_loss[-1],valid_loss[-1]))\n\nprint('Final train loss value: %.4f' % train_loss[-1])\n\n# Plot model performance\nepochs = range(num_epochs)\nplt.figure()\nplt.plot(epochs, train_loss, 'bo', label='Training loss')\nplt.plot(epochs, valid_loss, 'r-', label='Validation loss')\nplt.title('Training and validation losses')\nplt.legend()\nplt.savefig('AE_fraud_loss.png')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":562},"id":"gbVhkTJ_sr7Z","outputId":"34ae82b4-3cf6-4ab9-d083-8fb13f2302bf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Calculate autoencoder distances\n\nNow we calculate the euclidean distance between the autoencoder input and output.\n\n$$ \\text{distance} = \\sqrt{ ||x_{\\text{input}} - x_{\\text{output}}||^2} = \\sqrt{ \\sum_i (x^i_{\\text{input}} - x^i_{\\text{output}})^2}$$\n\na) See below how this is done for the normal test data, and do the same for the fraud test data.\n\nb) Plot the histograms of the calculated distances of the normal and fraud test data. For better viewing choose a logarithmic scale for the y axis. Comment on the result.","metadata":{"id":"XORZ0JVWsr7b"}},{"cell_type":"code","source":"model.eval() # Sets the module in evaluation mode.\nmodel.cpu()  # Moves all model parameters and buffers to the CPU to avoid out of memory\n\n# Normal test dataset\n#--------------------\ntest_normal_distance = []\nfor data in test_normal_loader:\n    data = data.type(dtype).cpu().detach()\n    output = model(data)\n    test_normal_distance += torch.sqrt((torch.sum((data-output)**2,axis=1)))\n\n# convert list to tensor\ntest_normal_distance = torch.FloatTensor(test_normal_distance)\n\n# convert tensor to numpy array\ntest_normal_distance = test_normal_distance.numpy()\n\n# Fraud test dataset\n#-------------------\ntest_fraud_distance = []\nfor data in test_fraud_loader:\n    data = data.type(dtype).cpu().detach()\n    output = model(data)\n    test_fraud_distance += torch.sqrt((torch.sum((data-output)**2,axis=1)))\n\n# convert list to tensor\ntest_fraud_distance = torch.FloatTensor(test_fraud_distance)\n\n# convert tensor to numpy array\ntest_fraud_distance = test_fraud_distance.numpy()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"Vn94wGThsr7b","outputId":"dccf9151-dc6a-472b-f5a9-e72fe86b529b","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bins = np.arange(0, 2, 0.01)\nplt.figure(figsize=(7, 7))\nn, bins, patches = plt.hist(test_normal_distance, bins=bins, alpha=0.5, label='Normal (Test)', density=1, log=True)\nplt.hist(test_fraud_distance, bins, alpha=0.5, label='Fraud (Test)', density=1)\nplt.xlabel('distance',fontsize=16)\nplt.legend(loc='upper right',fontsize=16)\nplt.savefig(\"distances.png\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":629},"id":"iHrIwrX4sr7c","outputId":"3da8dbbd-f0a1-44fc-bf11-2efab645ddcc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Confusion matrix\n\nBuild a confusion matrix with a threshold on the distance such that 50% of fraud transactions are detected. A confusion matrix is a table that summarizes the performance of a classification model by comparing its predicted labels to the true labels. It displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) of the model's predictions. What is the true positive rate in this case? Is this threshold interesting?","metadata":{"id":"Q5pFtFBCsr7d"}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nLABELS = [\"Normal\", \"Fraud\"]\n\ntarget = np.concatenate((np.zeros(x_test_normal.shape[0]),np.ones(x_test_fraud.shape[0])))\nscores = np.concatenate((test_normal_distance,test_fraud_distance))\n\nthreshold = 0.75\n\ny_pred = [1 if e > threshold else 0 for e in scores]\n\n# EXERCISE: Can you try to define the confusion matrix?\n# conf_matrix = confusion_matrix(,)\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()\n\nNN = conf_matrix[0,0] # True negative: Normal -> Normal\nNF = conf_matrix[0,1] # False positive: Normal -> Fraud\nFN = conf_matrix[1,0] # False negative: Fraud -> Normal\nFF = conf_matrix[1,1] # True positive: Fraud -> Fraud\n\nprint('False positive rate = %.2f %%' % (NF/(NN+NF)*100))\nprint('True positive rate = %.2f %%' % (FF/(FN+FF)*100))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":601},"id":"hTUeIU7xsr7d","outputId":"491d73ad-0d7c-4c28-f378-8f3e2cd1dfe5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. ROC Curve\n\nDraw the ROC curve for the test sample.","metadata":{"id":"cdk9_6J-sr7e"}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import cross_val_predict\n\ntarget = np.concatenate((np.zeros(x_test_normal.shape[0]),np.ones(x_test_fraud.shape[0])))\nscores = np.concatenate((test_normal_distance,test_fraud_distance))\n\nplt.figure(figsize=(7, 7))\n\nfp, vp, thresholds = roc_curve(target,scores,pos_label=1)\nroc_auc = auc(fp, vp)\n\nplt.plot(fp,vp,color='red',label='ROC curve %s (AUC = %0.4f)'%('AE',roc_auc))\n\nplt.xlabel('False Positive',fontsize=16)\nplt.ylabel('True Positive',fontsize=16)\nplt.plot([0, 1],[0, 1],\n         linestyle='--',color=(0.6, 0.6, 0.6),\n         label='Random guess')\nplt.plot([0, 0, 1],[0, 1, 1],color='yellow',label='Ideal')\nplt.grid()\nplt.legend(loc=\"best\",fontsize=16)\nplt.tight_layout()\nplt.savefig(\"ROC.png\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"OxFLwVOYsr7e","outputId":"33500b71-3d2e-46ae-8c91-d030af1ff43b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Further exercises\n\n## 1) What if you vary the autoencoder dimensions?\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2) How much effect can we see if we use another activation function?\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3) What if you vary the fraction between test and validation?","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}